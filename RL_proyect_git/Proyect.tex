\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{mathtools}
\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\usepackage[noabbrev,nameinlink]{cleveref}

\usepackage[style=apa,backend=biber]{biblatex} 
\addbibresource{bibliography.bib}

\title{RL\_Proyect}
\author{Carlos Soto (carlos.soto362@gmail.com)}
\date{June 2022}

\begin{document}

\maketitle

\section{Pure exchange with two commodities and two consumers or a Edgeworth box economy}
\textit{This section is based on \cite{mas-colell}}.\\
Pure exchange means no production. The simplest economy with profitable exchange with no production is the one with two commodities and two consumers. No production for this economy means that, the total amount of good $l$ that each of the consumers posses is less or equal than the initial amount of the good, with equality when there is no consumption and no waste. Denoting by $\{A,B\}$ the consumers, $x_{il}$ the total amount of good $l$ that consumer $i$ posses, $w_{il}$ the endowments, the initial amount of good $l$ that consumer $i$ posses and $\bar{w}_l$ the total amount of good $l$, no production, no consumption and no waste is represented by the equation,
\begin{equation}
\label{feasible}
    x_{Al} + x_{Bl} = w_{Al} + w_{Bl} = \bar{w}_l.
\end{equation}

The two tuples $\{(x_{A1},x_{A2}),(x_{B1},x_{B2})\}$ are called an ``Allocation'', all the Allocations that obey \cref{feasible} are called feasible allocations. Given a set of prices $(p_1,p_2)$, each consumer can exchange their initial goods with the budget constrain
\begin{equation}
    \label{budget}
    p_1x_{i1} + p_2x_{i2} = p_1w_{i1} + p_2w_{i2}.
\end{equation}
The consumers are going to exchange their goods in order to maximise their utility function $u_i(x_{i1},x_{i2})$. This is a function that describes the level of happiness that the consumer fills given an allocation. 

All the feasible allocations in this economy can be represented in the \textit{Edgeworth box} \cref{box1}, where the amount of good 1 that consumer $A$ posses is represented in the $x$ axis, and the amount of good 2 that consumer $A$ posses in the $y$ axis in the usual form, while the same applies for consumer $B$ but with the origin in the opposite corner. 

Given a utility function $u_A(x_{A1},x_{A2})$, a set of prices $(p_1,p_2)$ and an initial endowment $(w_{A1},w_{A2})$, there exist a feasible allocation such that the consumer $A$ maximise his utility function subject to his budget constrain. For an arbitrary set of prices, this allocation is different from the allocation that maximises $u_B(x_{B1},x_{B2})$. A Walraisan or competitive equilibrium for the Edgeworth box economy is a price vector $p^*$ and an Allocation, such that both utility functions are maximised under their respective budget constrains. 

\begin{figure}
    \centering
    \includegraphics[scale=0.25]{box1.png}
    \caption{Representation of the Edgeworth box, where one feasible allocation has been remarked.}
    \label{box1}
\end{figure}


\section{Translating the Edgeworth box economy to a Reinforcement Learning problem}

The Edgeworth box economy can be understood as a game, where two consumers exchange two goods, in order to maximise their utility function. Under complete knowledge of the utility functions, total amount of each good and initial endowments, it is possible to find the competitive equilibrium, so, under rationality assumptions, we could assume that the consumers would agree on the price of the goods as the price of this equilibrium, and exchange the necessary amount in order for they to maximise their utility function. However, this is far away from reality. Consumers rarely know how is the total amount of each good, the utility function of the other consumers and in most of the cases, they don't know their own utility function. This can be translated as a reinforcement learning (RL) problem with no knowledge of the world. 
\subsection*{Elements of the RL problem}
\subsubsection*{World}
The world consist of the utility functions of each consumer $u_A(x_{A1},x_{A2})$,\\ $u_B(x_{B1},x_{B2})$, and the set of all possible allocations and prices \\$\{ (x_{A1},x_{A2}),(x_{B1},x_{B2}),(p1,p2) \}$ such that \cref{feasible} is obeyed. 

To be able to work with the methods of reinforcement learning, this world is going to be discretized in the following manner,

for $n_1,n_2 \in \mathbf{N}$ , defined $\eta_1 = \bar{w}_1/n_1$,  $\eta_2 = \bar{w}_2/n_2$. Then, $p_1 = 1$, $p_2 \in \{\eta_2/\eta_1,2\eta_2/\eta_1, \hdots , \bar{w}_2/\eta_1 , \eta_2/2\eta_1,\hdots,\eta_2/\bar{w}_1 \}$, $x_{i1} \in \{\bar{w}_1,\bar{w}_1 - \eta_1 ,\hdots,0 \}$ and $x_{i2} \in \{\bar{w}_2,\bar{w}_2 - \eta_2 ,\hdots,0 \}$.

This discretisation transforms the Edgeworth box in a three dimensional greed world with $(n_1+1)(n_2+1)(n_1+n_2 + 1)$ points. The price $p_1$ can be set to one because what determines the competitive equilibrium is the ratio between the two prices.

In this world, there is only one point where equilibrium can be reached. 

\subsubsection*{Actions}
Without knowledge of their utility function, each consumer can only see the reward (the value of the utility function) after they have change the allocation. For doing so, they need to exchange with the other consumer. In order to work with this world in a similar manner than the greed world, each consumer can decide to buy or sell moving their allocation horizontally or vertically, but the movement is done only if both of the consumers agree on this decision. This agreement will be done in the following way:

If a consumer decide to sell an amount of good 2 according to his policy in the present point, he needs also to decide if he want to sell at the same, increase or decrease price, moving the state up or down in the price direction. Next, the other customer also needs to decide to buy at the new price, only if both of them agree on this, then they exchange giving the equivalent amount of good 1 in exchange of the amount of good 2. 

If both customers want to buy the good 2, nothing happens. If both customers want to sell the good 2, nothing happens.  

\subsubsection*{Rewards}
The rewards would be the value of their utility function after the action has been taken. The utility functions don't need to be the same for both customers. They are assumed to be concave functions of the goods. 

\subsection*{Possible simplification }
In case the dynamics don't manage to reach the competitive equilibrium because the learning algorithm is not well fitted for this problem or the problem is to big, one possible simplification could be to make the prices fix. The fixation of the prices would make that the competitive equilibrium may not exist, so, to ensure the existence, the problem could be set to $u_A(x_{A1},x_{A2})=u_B(x_{B1},x_{B2})$, and the initial budgets to be also equal, such that the competitive equilibrium would be $\{ (1/2,1/2),(1/2,1/2),(1,1)   \}$ by symmetry. 

\section{Algorithm}
\subsection*{Mathematical digression}
Each player is going to use an actor-critic algorithm, which is based on Natural Policy Gradient using an approximation of the value function as a baseline in order to learn the best strategy. Calling $\Pi_i(a,s)$ the policy of player $i$, the probability that player $i$ makes action $a$ given that she is in the state $s$, then the expected reward for player $i$ is
\begin{equation}
\begin{aligned}
    J_i &= \mathbf{E}[\sum_{t=0}^{\infty} \gamma^t R_{t+1}^{(i)}]\\&=\sum_{s,\hat{s},a_1,a_2,t}\gamma^tP(S_t = s)P(\hat{s}|s,a)\Pi_1(a_1|s)\Pi_2(a_2|s)u_i(s,a_1,a_2,\hat{s})
\end{aligned}
\end{equation}
where $\gamma$ is a discount factor over time, $R_{t+1}^{(i)}$ is the value of the utility function of player $i$ at time $t+1$, and $P$ stands for probability. 

The Natural Policy Gradient algorithm consist on maximising the function $J_i(\theta)$
by computing the sequence 
\begin{equation}
\label{eq:gradientD}
    \theta^{(n+1)} = \theta^{(n)} + \alpha \Tilde{\nabla}_{\theta^{(n)}}J_i(\theta^{(n)}) 
\end{equation}
with $\Tilde{\nabla}_\theta = F^{-1}\nabla_\theta$ the natural gradient, and $F$ the Fisher Information Matrix with elements 
\begin{equation}F_{\theta_a,\theta_b} = \sum_{c,s}\Pi_i(c|s)\frac{\partial \log{\Pi_i(c|s)}}{\partial \theta_{a}} \frac{\partial \log{\Pi_i(c|s)}}{\partial \theta_{b}}. \end{equation} This sequence, for suitable choice of $\alpha$, any given $\theta^{(0)}$ and concavity assumptions, would converge to 
\begin{equation}
    \theta^* = \argmax_\theta(J_i(\theta))
\end{equation}



It can be proven that, if the policy is parameterized by parameters $\theta$,
\begin{equation}
    \Tilde{\nabla}_\theta J_i = \sum_{t=0}^\infty \gamma^t\mathbf{E}[\left( \sum_{k=0}^\infty \gamma^k R_{t+k+1} \right) \Tilde{\nabla}_\theta \log{\Pi_i(A_t|S_t)}],
\end{equation}
where $A_t \in \mathbf{A}$ the action taken on time $t$, and $S_t \in \mathbf{S}$ the state on time $t$, then, by choosing the soft-max parametrization,
\begin{equation}
    \Pi_i(a,s) = \frac{e^{\theta_{a,s}}}{\sum_{a^*} e^{\theta_{a^*,s}}},
\end{equation}
\begin{equation}
\label{eq:naturalGradient}
    \Tilde{\nabla}_\theta J_i = \sum_{t=0}^\infty \gamma^t\mathbf{E}[\left( \sum_{k=0}^\infty \gamma^k R_{t+k+1} \right) \mathbf{I}(A_t,S_t)/\Pi_i(A_t|S_t)]
\end{equation}
with $\mathbf{I}(x)$ a vector of zeros for every action and state that has not been taken and one for the action and state in time $t$.

In order to decrease the variance and make the algorithm faster at learning, the Value Function is introduced as a baseline, 
\begin{equation}
    V_i(s) = \mathbf{E}[\sum_{t=0}^{\infty} \gamma^t R_{t+1}^{(i)}|S_0 = s]
\end{equation}
which obeys the Bellman's equation
\begin{equation}
    V(s) = \sum_{\hat{s},a} \Pi^{(t)}_i(a|\hat{s})P_t(\hat{s}|s,a)[u_i(s,a,\hat{s}) + \gamma V(\hat{s})].
\end{equation}
and the relation
\begin{equation}\label{eq:qbellmanseq}\begin{aligned}
    &Q_i(s,a) = \mathbf{E}[\sum_{t=0}^{\infty} \gamma^t R_{t+1}^{(i)}|S_0 = s,A_0 = a] \Rightarrow\\&V(s)= \sum_a \Pi^{(t)}_i(a|s)Q_i(s,a) \Rightarrow \\& Q(a,s) = \sum_{\hat{s}} P_t(\hat{s}|s,a)[u_i(s,a,\hat{s}) + \gamma V(\hat{s})].
\end{aligned}
\end{equation}
Using the \cref{eq:qbellmanseq} in \cref{eq:naturalGradient} and adding the Value Function as baseline, the resulting natural gradient is
\begin{equation}
\label{eq:ReinforceBaseline}
    \Tilde{\nabla}_\theta J_i =\mathbf{E}[ \sum_{t=0}^\infty \gamma^tu_i(S_{t+1},A_t,A_{t+1}) + \gamma V(S_{t+1}) - V(S_t))\mathbf{I}(A_t,S_t)/\Pi_i(A_t|S_t)].
\end{equation}

Dropping the Expected value in the previous equation, \cref{eq:ReinforceBaseline} and the \cref{eq:gradientD} give the ingredients for a stochastic policy gradient, which would converge for a suitable decreasing learning rate $\alpha=\alpha_t$. In order to reduce the variance and speed the algorithm, a bias is introduced by substituting the infinite sum with just one term of it. Because the Value function is also unknown, it also has to be learned. By choosing a parametric $\hat{V}(s,w)$, the intention is to minimize the distance between $V(s)$ and $\hat{V}(s,w)$, so defining the value error as 
\begin{equation}
    \bar{VE} = \sum_{s} \sum_{t=0}^\infty \gamma^tP(S_t=s)(V(s) - \hat{V}(s,w))^2,
\end{equation}
minimizing this value error by gradient decent, 
\begin{equation}
    \nabla (\bar{VE}) = -2\sum_{s} \sum_{t=0}^\infty \gamma^tP(S_t=s)(V(s) - \hat{V}(s,w))\nabla \hat{V}(s,w)
\end{equation}
which is approximated as 
\begin{equation}
\begin{aligned}
    \nabla (\bar{VE})_{aprox} &=  -2\sum_{s} \sum_{t=0}^\infty \gamma^tP(S_t=s)(u_i(S_{t+1}) +\gamma \hat{V}(S_{t+1},w) - \hat{V}(S_t,w))\nabla \hat{V}(S_t,w)\\&=-2\sum_{t=0}^\infty \mathbf{E}[\gamma^t(u_i(S_{t+1}) +\gamma \hat{V}(S_{t+1},w) - \hat{V}(S_t,w))\nabla \hat{V}(S_t,w)]
\end{aligned}
\end{equation}

Noticing that one state can be represented by three coordinates, $S = (S_1,S_2,S_3)$, then $\hat{V}(S,w)$ is chosen to be 
\begin{equation}
    \hat{V}(S,w) = w_1S_1 + w_2S_2 + w_3S_3,
\end{equation}
so, the final expression for the gradient of the value error that will be used is
\begin{equation}
    \nabla (\bar{VE})_{aprox}=-2\sum_{t=0}^\infty \mathbf{E}[\gamma^t(u_i(S_{t+1}) +\gamma \hat{V}(S_{t+1},w) - \hat{V}(S_t,w))S_t]
\end{equation}
Again, dropping the expected value give reach to a Stochastic gradient decent, and using only one term of the infinite sum increase the bias but can reduce the variance and speed up the algorithm. 

\subsection*{Finally, the algorithm: Actor-Critic}
\begin{itemize}
    \item[-] Input $\Pi_1(a|s)$, $\Pi_2(a|s)$, $\hat{V}_1(s,w_1)$, $\hat{V}_2(s,w_2)$, $\alpha_\theta$ and $\alpha_w$.
    \item[-] Initialize $\theta_1$, $\theta_2$, $w_1$ and $w_2$.
    \item[-] Loop over episodes:
    \begin{itemize}
        \item[-] Initialize a state $s$.
        \item[-] Loop over time:
        \begin{itemize}
            \item[-] Pick $A=(a_1,a_2)$ according to the policy $\Pi(A|s) = \Pi_1(a_1|s)\Pi_2(a_2|s)$.
            \item[-] Observe $\hat{s},u_1(\hat{s}),u_2(\hat{s}) $.
            \item[-] Compute the temporal difference error for each player $\delta_i = u_i(\hat{s}) + \gamma \hat{V}_i(\hat{s},w) - \hat{V}_i(s,w) $.
            \item[-] Modify the parameters, $w_i=w_i+\alpha_w \delta_i s $, $\theta_i = \theta_i + \alpha_\theta \delta_i \mathbf{I}(a,s)/\Pi_i(a|s)$, $s = \hat{s}$.
            
        \end{itemize}
    \end{itemize}
    
\end{itemize}

\printbibliography
\end{document}

